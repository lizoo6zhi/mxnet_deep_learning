{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata\n",
    "from sklearn import datasets\n",
    "from mxnet import nd,autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "(1797, 10)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "digits = datasets.load_digits()\n",
    "features,labels = nd.array(digits['data']),nd.array(digits['target'])\n",
    "print(features.shape,labels.shape)\n",
    "labels_onehot = nd.one_hot(labels,10)\n",
    "print(labels_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmaxClassifier:\n",
    "    def __init__(self,inputs,outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        self.weight = nd.random.normal(scale=0.01,shape=(inputs,outputs))\n",
    "        self.bias = nd.zeros(shape=(1,outputs))\n",
    "        self.weight.attach_grad()\n",
    "        self.bias.attach_grad()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = nd.dot(x,self.weight) + self.bias\n",
    "        return self._softmax(output)\n",
    "        \n",
    "    def _softmax(self,x):\n",
    "        step1 = x.exp()\n",
    "        step2 = step1.sum(axis=1,keepdims=True)\n",
    "        return step1 / step2\n",
    "    \n",
    "    def _bgd(self,params,learning_rate,batch_size):\n",
    "        '''\n",
    "        批量梯度下降\n",
    "        '''\n",
    "        for param in params:       # 直接使用mxnet的自动求梯度\n",
    "            param[:] = param - param.grad * learning_rate / batch_size\n",
    "            \n",
    "    def loss(self,y_pred,y):\n",
    "        return nd.sum((-y * y_pred.log())) / len(y)\n",
    "            \n",
    "    def dataIter(self,x,y,batch_size):\n",
    "        dataset = gdata.ArrayDataset(x,y)\n",
    "        return gdata.DataLoader(dataset,batch_size,shuffle=True)\n",
    "    \n",
    "    def fit(self,x,y,learning_rate,epoches,batch_size):\n",
    "        for epoch in range(epoches):\n",
    "            for x_batch,y_batch in self.dataIter(x,y,batch_size):\n",
    "                with autograd.record():\n",
    "                    y_pred = self.forward(x_batch)\n",
    "                    l = self.loss(y_pred,y_batch)\n",
    "                l.backward()\n",
    "                self._bgd([self.weight,self.bias],learning_rate,batch_size)\n",
    "            if epoch % 50 == 0:\n",
    "                y_all_pred = self.forward(x)\n",
    "                print('epoch:{},loss:{},accuracy:{}'.format(epoch+50,self.loss(y_all_pred,y),self.accuracyScore(y_all_pred,y)))\n",
    "            \n",
    "    def predict(self,x):\n",
    "        y_pred = self.forward(x)\n",
    "        return y_pred.argmax(axis=0)\n",
    "    \n",
    "    def accuracyScore(self,y_pred,y):\n",
    "        acc_sum = (y_pred.argmax(axis=1) == y.argmax(axis=1)).sum().asscalar()\n",
    "        return acc_sum / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50,loss:\n",
      "[1.9941667]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.3550361713967724\n",
      "epoch:100,loss:\n",
      "[0.37214527]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9393433500278241\n",
      "epoch:150,loss:\n",
      "[0.25443634]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9549248747913188\n",
      "epoch:200,loss:\n",
      "[0.20699367]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9588202559821926\n",
      "epoch:250,loss:\n",
      "[0.1799827]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9660545353366722\n",
      "epoch:300,loss:\n",
      "[0.1619963]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9677239844184753\n",
      "epoch:350,loss:\n",
      "[0.14888664]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9716193656093489\n",
      "epoch:400,loss:\n",
      "[0.13875261]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9738452977184195\n",
      "epoch:450,loss:\n",
      "[0.13058177]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9760712298274903\n",
      "epoch:500,loss:\n",
      "[0.12379646]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9777406789092933\n"
     ]
    }
   ],
   "source": [
    "sfm_clf = softmaxClassifier(64,10)\n",
    "sfm_clf.fit(features,labels_onehot,learning_rate=0.1,epoches=500,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果： \n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "真实结果： \n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('预测结果：',sfm_clf.predict(features[:10]))\n",
    "print('真实结果：',labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用mxnet实现softmax分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50,loss:\n",
      "[2.1232333]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.24652198107957707\n",
      "epoch:100,loss:\n",
      "[0.37193483]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9410127991096272\n",
      "epoch:150,loss:\n",
      "[0.25408813]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9543683917640512\n",
      "epoch:200,loss:\n",
      "[0.20680156]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9627156371730662\n",
      "epoch:250,loss:\n",
      "[0.1799252]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9666110183639399\n",
      "epoch:300,loss:\n",
      "[0.16203885]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9699499165275459\n",
      "epoch:350,loss:\n",
      "[0.14899409]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9738452977184195\n",
      "epoch:400,loss:\n",
      "[0.13890252]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9749582637729549\n",
      "epoch:450,loss:\n",
      "[0.13076076]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9755147468002225\n",
      "epoch:500,loss:\n",
      "[0.1239901]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9777406789092933\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon,nd,autograd,init\n",
    "from mxnet.gluon import nn,trainer,loss as gloss,data as gdata\n",
    "# 定义模型\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10))\n",
    "\n",
    "# 初始化模型\n",
    "net.initialize(init=init.Normal(sigma=0.01))\n",
    "\n",
    "# 损失函数\n",
    "loss = gloss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "# 优化算法\n",
    "optimizer = trainer.Trainer(net.collect_params(),'sgd',{'learning_rate':0.1})\n",
    "\n",
    "# 训练\n",
    "epoches = 500\n",
    "batch_size = 200\n",
    "\n",
    "dataset = gdata.ArrayDataset(features, labels_onehot)\n",
    "data_iter = gdata.DataLoader(dataset,batch_size,shuffle=True)\n",
    "for epoch in range(epoches):\n",
    "    for x_batch,y_batch in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net.forward(x_batch), y_batch).sum() / batch_size\n",
    "        l.backward()\n",
    "        optimizer.step(batch_size)\n",
    "    if epoch % 50 == 0:\n",
    "        y_all_pred = net.forward(features)\n",
    "        acc_sum = (y_all_pred.argmax(axis=1) == labels_onehot.argmax(axis=1)).sum().asscalar()\n",
    "        print('epoch:{},loss:{},accuracy:{}'.format(epoch+50,loss(y_all_pred,labels_onehot).sum() / len(labels_onehot),acc_sum/len(y_all_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
