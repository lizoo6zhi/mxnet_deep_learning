{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>需求:</b>\n",
    "- 从零和使用mxnet实现dropout\n",
    "\n",
    "<b>数据集：</b>\n",
    "- 使用load_digits()手写数字数据集\n",
    "\n",
    "<b>要求：</b>\n",
    "- 使用1个掩藏层n_hidden1 = 36，激活函数为relu，损失函数为softmax交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from mxnet import gluon,nd,autograd,init\n",
    "from mxnet.gluon import nn,data as gdata,loss as gloss,trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "(1797, 10)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "digits = datasets.load_digits()\n",
    "features,labels = nd.array(digits['data']),nd.array(digits['target'])\n",
    "print(features.shape,labels.shape)\n",
    "labels_onehot = nd.one_hot(labels,10)\n",
    "print(labels_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuroNet:\n",
    "    def __init__(self,n_inputs,n_hidden1,n_outputs):\n",
    "        hidden_layer = Layer(n_inputs,n_hidden1)\n",
    "        output_layer = Layer(n_hidden1,n_outputs)\n",
    "        self.layers = [hidden_layer,output_layer]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for param in layer.params:\n",
    "                param.attach_grad()\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        step1 = x.exp()\n",
    "        step2 = step1 / step1.sum(axis=1,keepdims=True)\n",
    "        return step2\n",
    "    \n",
    "    def softmaxCrossEntropyLoss(self,y_pred,y):\n",
    "        step1 = -y * y_pred.log()\n",
    "        step2 = step1.sum(axis=1)\n",
    "        loss = step2.sum(axis=0) / len(y)\n",
    "        return loss\n",
    "        \n",
    "    def drop(self,x,drop_probability,train=True):\n",
    "        '''\n",
    "        神经元被丢弃的概率为p\n",
    "        '''\n",
    "        if train:\n",
    "            mask = nd.random.uniform(0,1,shape=x.shape,dtype='float32') > drop_probability\n",
    "            return mask * x / (1 - drop_probability)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self,x,train=True):\n",
    "        for layer in self.layers[:-1]:\n",
    "            step1 = layer.forward(x)\n",
    "            step2 = self.drop(step1,0.2,train)\n",
    "            x = step2\n",
    "        output_layer = self.layers[-1]\n",
    "        return self.softmax(output_layer.forward(x))\n",
    "    \n",
    "    def sgd(self,learning_rate,batch_size):\n",
    "        '''\n",
    "        使用随机梯度下降更新所有权重和偏置\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            layer.sgd(learning_rate,batch_size)\n",
    "                \n",
    "    def dataIter(self,x,y,batch_size):\n",
    "        dataset = gdata.ArrayDataset(x,y)\n",
    "        return gdata.DataLoader(dataset,batch_size,shuffle=True)\n",
    "    \n",
    "    def fit(self,x,y,epoches,batch_size,learning_rate):\n",
    "        for epoch in range(epoches):\n",
    "            for x_batch,y_batch in self.dataIter(x,y,batch_size):\n",
    "                with autograd.record():\n",
    "                    y_pred = self.forward(x_batch,train=True)\n",
    "                    loss = self.softmaxCrossEntropyLoss(y_pred,y_batch)\n",
    "                loss.backward()\n",
    "                self.sgd(learning_rate,batch_size)\n",
    "            if epoch % 50 == 0:\n",
    "                y_pred_all = self.forward(x,train=False)\n",
    "                loss_all = self.softmaxCrossEntropyLoss(y_pred_all,y)\n",
    "                accuracy_score = self.accuracyScore(y_pred_all,y)\n",
    "                print('epoch:{},loss:{},accuracy:{}'.format(epoch+50,loss_all,accuracy_score))\n",
    "            \n",
    "    def predict(self,x):\n",
    "        y_pred = self.forward(x)\n",
    "        return y_pred.argmax(axis=0)\n",
    "    \n",
    "    def accuracyScore(self,y_pred,y):\n",
    "        acc_sum = (y_pred.argmax(axis=1) == y.argmax(axis=1)).sum().asscalar()\n",
    "        return acc_sum / len(y)\n",
    "        \n",
    "class Layer:\n",
    "    def __init__(self,n_inputs,n_outputs):\n",
    "        weight = nd.random.normal(scale=0.01,shape=(n_inputs,n_outputs))\n",
    "        bias = nd.zeros(shape=(n_outputs))\n",
    "        self.params = [weight,bias]\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return nd.maximum(x,0)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        step1 = nd.dot(x,self.params[0]) + self.params[1]\n",
    "        return self.relu(step1)\n",
    "    \n",
    "    def sgd(self,learning_rate,batch_size):\n",
    "        for param in self.params:\n",
    "            param[:] = param - learning_rate * param.grad / batch_size \n",
    "            \n",
    "    def print_params(self):\n",
    "        for param in self.params:\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50,loss:\n",
      "[2.2988722]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.18308291597106288\n",
      "epoch:100,loss:\n",
      "[1.4126126]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.7395659432387313\n",
      "epoch:150,loss:\n",
      "[0.46316707]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9259877573734001\n",
      "epoch:200,loss:\n",
      "[0.24678323]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9493600445186422\n",
      "epoch:250,loss:\n",
      "[0.17839472]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9610461880912632\n",
      "epoch:300,loss:\n",
      "[0.14298467]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9688369504730105\n",
      "epoch:350,loss:\n",
      "[0.1198809]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9738452977184195\n",
      "epoch:400,loss:\n",
      "[0.10388324]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9782971619365609\n",
      "epoch:450,loss:\n",
      "[0.0917427]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9827490261547023\n",
      "epoch:500,loss:\n",
      "[0.08237094]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9849749582637729\n"
     ]
    }
   ],
   "source": [
    "net = NeuroNet(64,36,10)\n",
    "net.fit(features,labels_onehot,epoches=500,batch_size=200,learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果： \n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "真实结果： \n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('预测结果：',net.predict(features[:10]))\n",
    "print('真实结果：',labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用mxnet实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50,loss:\n",
      "[2.2981045]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.16304952698942682\n",
      "epoch:100,loss:\n",
      "[0.97166663]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.867557039510295\n",
      "epoch:150,loss:\n",
      "[0.3836201]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9243183082915971\n",
      "epoch:200,loss:\n",
      "[0.24329802]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9449081803005008\n",
      "epoch:250,loss:\n",
      "[0.18068495]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9577072899276572\n",
      "epoch:300,loss:\n",
      "[0.14546551]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9660545353366722\n",
      "epoch:350,loss:\n",
      "[0.1219953]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9727323316638843\n",
      "epoch:400,loss:\n",
      "[0.10563282]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9760712298274903\n",
      "epoch:450,loss:\n",
      "[0.09357208]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9788536449638287\n",
      "epoch:500,loss:\n",
      "[0.08368526]\n",
      "<NDArray 1 @cpu(0)>,accuracy:0.9816360601001669\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 64\n",
    "n_hiddens = 36\n",
    "n_outputs = 10\n",
    "\n",
    "# 定义模型\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(n_hiddens,activation='relu'))\n",
    "net.add(nn.Dropout(rate=0.2))\n",
    "net.add(nn.Dense(n_outputs))\n",
    "\n",
    "# 初始化模型\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "# 损失函数\n",
    "loss = gloss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "\n",
    "optimizer = trainer.Trainer(net.collect_params(), 'sgd', {'learning_rate':0.5})\n",
    "\n",
    "# 训练模型\n",
    "epoches = 500\n",
    "batch_size = 200\n",
    "\n",
    "dataset = gdata.ArrayDataset(features,labels_onehot)\n",
    "dataIter = gdata.DataLoader(dataset,batch_size,shuffle=True)\n",
    "for epoch in range(epoches):\n",
    "    for x_batch,y_batch in dataIter:\n",
    "        with autograd.record():\n",
    "            y_pred = net.forward(x_batch)\n",
    "            l = loss(y_pred, y_batch).sum() / batch_size\n",
    "        l.backward()\n",
    "        optimizer.step(batch_size)\n",
    "    if epoch % 50 == 0:\n",
    "        y_all_pred = net.forward(features)\n",
    "        acc_sum = (y_all_pred.argmax(axis=1) == labels_onehot.argmax(axis=1)).sum().asscalar()\n",
    "        print('epoch:{},loss:{},accuracy:{}'.format(epoch+50,loss(y_all_pred,labels_onehot).sum() / len(labels_onehot),acc_sum/len(y_all_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
